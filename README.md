#  طبقه‌بند مدرن CNN با خط لوله بهینه‌ی `tf.data`

این پروژه یک طبقه‌بند تصویر سگ و گربه با استفاده از یک شبکه‌ی عصبی کانولوشنی (CNN) است، اما با تمرکز بر **روش‌های مدرن و بهینه‌ی TensorFlow 2.x**.

برخلاف استفاده از `ImageDataGenerator` قدیمی، این پروژه از خط لوله‌ی داده‌ی `tf.data` برای بارگذاری، پیش‌پردازش و تغذیه‌ی داده‌ها به GPU به صورت بهینه استفاده می‌کند.

**نتیجه نهایی: این مدل با استفاده از `EarlyStopping` برای جلوگیری از Overfitting، به دقت واقعی ۸۲٪ بر روی داده‌های آزمایشی دست یافت.**

---

##  دیتاست (Dataset)

این پروژه از دیتاست معروف **"Dogs vs. Cats"** که در رقابت Kaggle معرفی شد، استفاده می‌کند.

* **منبع:** شما می‌توانید دیتاست کامل (train و test) را مستقیماً از [**وب‌سایت مایکروسافت (لینک دانلود)**](https://www.microsoft.com/en-us/download/details.aspx?id=54765) دانلود کنید.
* **ساختار مورد نیاز:** کد انتظار دارد که داده‌ها در پوشه‌ای به نام `dataset/` با این ساختار قرار گیرند:
    ```
    dataset/
    ├── training_set/
    │   ├── cats/
    │   └── dogs/
    └── test_set/
        ├── cats/
        └── dogs/
    ```
---

## ⚙️ فرآیند و متدولوژی (Workflow)

### ۱. خط لوله‌ی داده بهینه (`tf.data`)
به جای `ImageDataGenerator`، از `tf.keras.utils.image_dataset_from_directory` برای بارگذاری مستقیم داده‌ها از دیسک استفاده شد.

### ۲. بهینه‌سازی عملکرد (Caching & Prefetching)
برای به حداکثر رساندن توان GPU و جلوگیری از «گلوگاه» (bottleneck) ورودی/خروجی، خط لوله‌ی داده با دو تابع کلیدی بهینه شد:
* **`.cache()`**: داده‌ها پس از اولین بار خوانده شدن از دیسک، در حافظه‌ی RAM کش می‌شوند تا در Epochهای بعدی با سرعت بالا در دسترس باشند.
* **`.prefetch(AUTOTUNE)`**: به CPU اجازه می‌دهد تا دسته‌ی (Batch) بعدی داده‌ها را **همزمان** که GPU در حال پردازش دسته‌ی فعلی است، آماده کند.

### ۳. پیش‌پردازش و افزایش داده (Layers)
* **نرمال‌سازی:** یک لایه‌ی `layers.Rescaling(1./255)` به عنوان بخشی از مدل (یا در تابع `prepare`) برای نرمال‌سازی پیکسل‌ها استفاده شد.
* **افزایش داده (Augmentation):** لایه‌های `RandomFlip`, `RandomRotation`, و `RandomZoom` در تابع `prepare` تعریف شدند تا فقط بر روی داده‌های آموزشی اعمال شوند.

### ۴. معماری مدل و رگولاریزیشن
مدل CNN شامل ۳ بلوک `Conv2D` و `MaxPool2D` بود که به یک سر طبقه‌بند (`Flatten`, `Dense`, `Dropout`) متصل می‌شد.
* **`Dropout(0.25)`**: برای جلوگیری از Overfitting، یک لایه‌ی Dropout با نرخ ۲۵٪ استفاده شد.

### ۵. آموزش هوشمند با `EarlyStopping`
برای پیدا کردن بهترین مدل و جلوگیری از اتلاف وقت و Overfitting:
* **`EarlyStopping`**: آموزش به صورت خودکار متوقف شد، زمانی که `val_loss` (خطای تست) برای **۱۰ دور (patience=10)** متوالی بهبودی پیدا نکرد.
* **`restore_best_weights=True`**: مدل نهایی به صورت خودکار به وزن‌های **بهترین Epoch** (نه آخرین Epoch) بازگردانده شد، که منجر به دقت **۸۲٪** شد.

---

##  کتابخانه‌های استفاده‌شده

* **TensorFlow 2.x** (`tf.data`, `tf.keras`)
* **Scikit-learn** (برای `confusion_matrix` و `classification_report`)
* **Numpy**
* **Matplotlib**

---

##  نحوه اجرا (How to Run)

1.  این ریپازیتوری را `clone` کنید.
2.  دیتاست را از [لینک مایکروسافت](https://www.microsoft.com/en-us/download/details.aspx?id=54765) دانلود کنید.
3.  پوشه‌های `training_set` و `test_set` را دقیقاً طبق ساختاری که در بخش دیتاست توضیح داده شد، در پوشه‌ی `dataset` قرار دهید.
4.  نوت‌بوک جوپیتر (`.ipynb`) را اجرا کنید.
